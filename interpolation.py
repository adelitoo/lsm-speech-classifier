import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import make_interp_spline, UnivariateSpline
from scipy.signal import savgol_filter

# Your experimental data (parsed from results)
weights = np.array([
    0.001000, 0.001118, 0.001236, 0.001355, 0.001473, 0.001591, 0.001709, 0.001828,
    0.001946, 0.002064, 0.002182, 0.002301, 0.002419, 0.002537, 0.002655, 0.002774,
    0.002892, 0.003010, 0.003128, 0.003246, 0.003365, 0.003483, 0.003601, 0.003719,
    0.003838, 0.003956, 0.004074, 0.004192, 0.004311, 0.004429, 0.004547, 0.004665,
    0.004784, 0.004902, 0.005020, 0.005138, 0.005257, 0.005375, 0.005493, 0.005611,
    0.005729, 0.005848, 0.005966, 0.006084, 0.006202, 0.006321, 0.006439, 0.006557,
    0.006675, 0.006794, 0.006912, 0.007030, 0.007148, 0.007267, 0.007385, 0.007503,
    0.007621, 0.007739, 0.007858, 0.007976, 0.008094, 0.008212, 0.008331, 0.008449,
    0.008567, 0.008685, 0.008804, 0.008922, 0.009040, 0.009158, 0.009277, 0.009395,
    0.009513, 0.009631, 0.009749, 0.009868, 0.009986, 0.010104, 0.010222, 0.010341,
    0.010459, 0.010577, 0.010695, 0.010814, 0.010932, 0.011050, 0.011168, 0.011287,
    0.011405, 0.011523, 0.011641, 0.011760, 0.011878, 0.011996, 0.012114, 0.012232,
    0.012351, 0.012469, 0.012587, 0.012705, 0.012824, 0.012942, 0.013060, 0.013178,
    0.013297, 0.013415, 0.013533, 0.013651, 0.013770, 0.013888, 0.014006, 0.014124,
    0.014242, 0.014361, 0.014479, 0.014597, 0.014715, 0.014834, 0.014952, 0.015070,
    0.015188, 0.015307, 0.015425, 0.015543, 0.015661, 0.015780, 0.015898, 0.016016,
    0.016134, 0.016253, 0.016371, 0.016489, 0.016607, 0.016725, 0.016844, 0.016962,
    0.017080, 0.017198, 0.017317, 0.017435, 0.017553, 0.017671, 0.017790, 0.017908,
    0.018026, 0.018144, 0.018263, 0.018381, 0.018499, 0.018617, 0.018735, 0.018854,
    0.018972, 0.019090, 0.019208, 0.019327, 0.019445, 0.019563, 0.019681, 0.019800,
    0.019918, 0.020036, 0.020154, 0.020273, 0.020391, 0.020509, 0.020627, 0.020745,
    0.020864, 0.020982, 0.021100, 0.021218, 0.021337, 0.021455, 0.021573, 0.021691,
    0.021810, 0.021928, 0.022046, 0.022164, 0.022283, 0.022401, 0.022519, 0.022637,
    0.022756, 0.022874, 0.022992, 0.023110, 0.023228, 0.023347, 0.023465, 0.023583,
    0.023701, 0.023820, 0.023938, 0.024056, 0.024174, 0.024293, 0.024411, 0.024529,
    0.024647, 0.024766, 0.024884, 0.025002, 0.025120, 0.025238, 0.025357, 0.025475,
    0.025593, 0.025711, 0.025830, 0.025948, 0.026066, 0.026184, 0.026303, 0.026421,
    0.026539, 0.026657, 0.026776, 0.026894, 0.027012, 0.027130, 0.027248, 0.027367,
    0.027485, 0.027603, 0.027721, 0.027840, 0.027958, 0.028076, 0.028194, 0.028313,
    0.028431, 0.028549, 0.028667, 0.028786, 0.028904, 0.029022, 0.029140, 0.029259,
    0.029377, 0.029495, 0.029613, 0.029731, 0.029850, 0.029968, 0.030086, 0.030204,
    0.030323, 0.030441, 0.030559, 0.030677, 0.030796, 0.030914, 0.031032, 0.031150,
    0.031269, 0.031387, 0.031505, 0.031623, 0.031741, 0.031860, 0.031978, 0.032096,
    0.032214, 0.032333, 0.032451, 0.032569, 0.032687, 0.032806, 0.032924, 0.033042,
    0.033160, 0.033279, 0.033397, 0.033515, 0.033633, 0.033752, 0.033870, 0.033988,
    0.034106, 0.034224, 0.034343, 0.034461, 0.034579, 0.034697, 0.034816, 0.034934,
    0.035052, 0.035170, 0.035289, 0.035407, 0.035525, 0.035643, 0.035762, 0.035880,
    0.035998, 0.036116, 0.036234, 0.036353, 0.036471, 0.036589, 0.036707, 0.036826,
    0.036944, 0.037062, 0.037180, 0.037299, 0.037417, 0.037535, 0.037653, 0.037772,
    0.037890, 0.038008, 0.038126, 0.038244, 0.038363, 0.038481, 0.038599, 0.038717,
    0.038836, 0.038954, 0.039072, 0.039190, 0.039309, 0.039427, 0.039545, 0.039663,
    0.039782, 0.039900, 0.040018, 0.040136, 0.040255, 0.040373, 0.040491, 0.040609,
    0.040727, 0.040846, 0.040964, 0.041082, 0.041200, 0.041319, 0.041437, 0.041555,
    0.041673, 0.041792, 0.041910, 0.042028, 0.042146, 0.042265, 0.042383, 0.042501,
    0.042619, 0.042737, 0.042856, 0.042974, 0.043092, 0.043210, 0.043329, 0.043447,
    0.043565, 0.043683, 0.043802, 0.043920, 0.044038, 0.044156, 0.044275, 0.044393,
    0.044511, 0.044629, 0.044747, 0.044866, 0.044984, 0.045102, 0.045220, 0.045339,
    0.045457, 0.045575, 0.045693, 0.045812, 0.045930, 0.046048, 0.046166, 0.046285,
    0.046403, 0.046521, 0.046639, 0.046758, 0.046876, 0.046994, 0.047112, 0.047230,
    0.047349, 0.047467, 0.047585, 0.047703, 0.047822, 0.047940, 0.048058, 0.048176,
    0.048295, 0.048413, 0.048531, 0.048649, 0.048768, 0.048886, 0.049004, 0.049122,
    0.049240, 0.049359, 0.049477, 0.049595, 0.049713, 0.049832, 0.049950, 0.050068,
    0.050186, 0.050305, 0.050423, 0.050541, 0.050659, 0.050778, 0.050896, 0.051014,
    0.051132, 0.051251, 0.051369, 0.051487, 0.051605, 0.051723, 0.051842, 0.051960,
    0.052078, 0.052196, 0.052315, 0.052433, 0.052551, 0.052669, 0.052788, 0.052906,
    0.053024, 0.053142, 0.053261, 0.053379, 0.053497, 0.053615, 0.053733, 0.053852,
    0.053970, 0.054088, 0.054206, 0.054325, 0.054443, 0.054561, 0.054679, 0.054798,
    0.054916, 0.055034, 0.055152, 0.055271, 0.055389, 0.055507, 0.055625, 0.055743,
    0.055862, 0.055980, 0.056098, 0.056216, 0.056335, 0.056453, 0.056571, 0.056689,
    0.056808, 0.056926, 0.057044, 0.057162, 0.057281, 0.057399, 0.057517, 0.057635,
    0.057754, 0.057872, 0.057990, 0.058108, 0.058226, 0.058345, 0.058463, 0.058581,
    0.058699, 0.058818, 0.058936, 0.059054, 0.059172, 0.059291, 0.059409, 0.059527,
    0.059645, 0.059764, 0.059882, 0.060000
])

accuracies = np.array([
    49.50, 43.50, 44.25, 50.00, 58.75, 58.50, 51.50, 56.75, 51.00, 43.50,
    46.50, 36.50, 56.25, 50.50, 47.25, 54.75, 57.00, 62.75, 58.25, 54.75,
    45.50, 53.50, 54.00, 54.00, 58.75, 52.50, 62.25, 56.25, 59.00, 63.25,
    63.50, 62.25, 64.25, 58.00, 62.50, 60.50, 59.25, 56.25, 64.00, 60.00,
    64.50, 60.00, 59.50, 61.75, 58.50, 65.75, 66.50, 61.50, 60.50, 60.00,
    60.00, 63.50, 66.25, 67.25, 62.50, 63.75, 65.50, 64.50, 67.25, 68.00,
    69.00, 67.50, 61.00, 67.75, 64.00, 62.00, 70.25, 69.50, 66.75, 66.25,
    65.50, 68.25, 68.50, 70.50, 65.50, 64.25, 66.00, 65.25, 65.50, 70.00,
    69.00, 66.50, 65.50, 70.00, 67.25, 68.00, 68.75, 67.50, 66.75, 71.50,
    70.25, 68.00, 65.25, 67.00, 68.75, 69.50, 68.25, 67.75, 67.75, 65.50,
    69.75, 64.50, 67.00, 67.75, 65.50, 72.75, 70.50, 68.25, 68.25, 66.00,
    68.25, 67.00, 69.25, 67.25, 67.50, 68.50, 66.50, 68.50, 68.75, 69.00,
    68.25, 69.25, 68.50, 68.50, 67.25, 67.50, 69.25, 68.25, 68.50, 70.00,
    71.00, 64.25, 70.00, 66.25, 66.75, 71.00, 69.75, 66.75, 70.00, 71.50,
    67.25, 69.25, 68.75, 68.50, 68.50, 67.25, 67.50, 67.50, 65.75, 69.50,
    69.75, 70.00, 67.50, 66.75, 68.25, 70.75, 70.00, 70.75, 67.75, 67.00,
    68.00, 71.25, 67.00, 69.50, 67.00, 70.25, 71.00, 71.75, 69.75, 73.25,
    68.25, 68.25, 70.50, 67.25, 68.75, 68.50, 68.75, 67.00, 67.50, 69.50,
    70.25, 65.75, 66.50, 68.75, 70.50, 69.00, 69.50, 72.25, 70.50, 70.25,
    70.50, 70.75, 68.50, 65.75, 71.75, 69.25, 65.75, 68.00, 69.25, 69.25,
    68.75, 68.75, 67.25, 70.25, 69.00, 69.50, 66.25, 68.75, 66.25, 67.25,
    68.75, 68.75, 70.25, 70.75, 66.00, 68.50, 70.50, 71.25, 68.00, 67.00,
    71.50, 66.75, 69.50, 70.75, 67.75, 69.25, 67.50, 66.50, 67.25, 67.50,
    68.75, 68.75, 67.50, 69.00, 67.25, 67.50, 67.00, 68.75, 66.50, 67.00,
    67.25, 67.25, 67.50, 70.50, 65.25, 69.75, 68.50, 67.25, 67.50, 66.00,
    70.25, 63.50, 67.25, 69.75, 67.50, 70.00, 69.00, 68.75, 66.75, 65.75,
    67.25, 70.00, 67.50, 66.25, 66.75, 69.50, 69.25, 66.50, 67.00, 67.00,
    67.00, 66.00, 68.00, 68.75, 67.00, 66.50, 68.50, 65.75, 70.00, 68.00,
    69.00, 68.50, 68.25, 64.25, 66.75, 66.50, 64.50, 67.00, 68.00, 65.25,
    66.50, 67.75, 64.00, 67.00, 69.50, 67.00, 66.75, 65.75, 64.25, 68.25,
    65.75, 65.75, 67.00, 65.50, 67.50, 66.00, 66.00, 68.25, 66.25, 63.00,
    65.00, 66.75, 67.00, 64.00, 67.00, 61.25, 65.00, 64.25, 64.50, 62.75,
    62.75, 63.50, 62.50, 60.50, 63.75, 61.25, 60.75, 57.75, 63.25, 63.00,
    59.50, 60.50, 58.00, 60.00, 61.25, 60.75, 59.50, 58.25, 57.00, 59.00,
    57.00, 57.75, 57.25, 57.50, 58.00, 53.25, 55.00, 57.00, 55.25, 52.75,
    53.00, 52.50, 55.00, 51.25, 53.50, 50.75, 50.25, 50.00, 49.50, 46.25,
    45.75, 46.50, 45.00, 43.50, 41.75, 39.00, 41.25, 38.50, 35.50, 39.50,
    42.00, 31.50, 33.50, 34.25, 34.25, 31.00, 34.25, 33.50, 34.25, 37.50,
    31.75, 35.50, 35.50, 41.50, 37.00, 35.75, 32.50, 34.75, 35.00, 39.25,
    35.75, 40.75, 36.00, 33.50, 35.50, 34.50, 35.00, 36.50, 35.25, 35.50,
    35.00, 35.25, 40.25, 38.25, 38.75, 39.25, 39.00, 36.50, 39.25, 35.25,
    39.00, 36.50, 37.00, 37.75, 36.25, 33.50, 34.75, 37.00, 36.50, 35.50,
    35.00, 39.00, 35.50, 40.25, 39.75, 38.25, 39.50, 35.50, 37.25, 36.50,
    37.25, 35.00, 36.00, 38.75, 33.25, 36.75, 35.00, 40.25, 37.50, 43.25,
    36.00, 41.25, 39.00, 37.75, 39.75, 41.75, 40.50, 43.25, 38.75, 38.25,
    39.00, 41.50, 41.25, 37.00, 37.00, 40.25, 38.25, 38.00, 40.25, 40.00,
    35.75, 38.50, 38.75, 34.50, 38.00, 36.50, 39.00, 37.25, 43.00, 39.50,
    30.25, 44.00, 39.75, 37.50, 38.25, 38.50, 38.00, 37.25, 38.75, 35.00,
    36.75, 38.00, 39.50, 35.50, 39.50, 34.75, 38.75, 40.25, 39.00, 39.75,
    35.00, 43.25, 41.00, 37.00, 36.50, 42.00, 33.50, 38.25, 33.75, 38.50
]) / 100.0  # Convert to decimal

# Reference w_critico
w_critico = 0.024302

# Find best accuracy
best_idx = np.argmax(accuracies)
best_w = weights[best_idx]
best_acc = accuracies[best_idx]

# Create figure with multiple subplots comparing smoothing methods
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Original data
ax = axes[0, 0]
ax.plot(weights, accuracies, 'o-', alpha=0.5, markersize=3, label="Original Data")
ax.axvline(x=w_critico, color='r', linestyle='--', alpha=0.7, label=f'w_critico ({w_critico:.6f})')
ax.plot(best_w, best_acc, 'r*', markersize=15, label=f'Best: {best_acc*100:.2f}%')
ax.set_title("Original Data", fontsize=14, fontweight='bold')
ax.set_xlabel("Average Synaptic Weight")
ax.set_ylabel("Test Accuracy")
ax.legend()
ax.grid(True, alpha=0.3)

# 2. Savitzky-Golay Filter (good for preserving features)
ax = axes[0, 1]
window = 51  # Must be odd
poly_order = 3
smoothed_savgol = savgol_filter(accuracies, window, poly_order)
ax.plot(weights, accuracies, 'o', alpha=0.3, markersize=2, label="Original", color='lightblue')
ax.plot(weights, smoothed_savgol, '-', linewidth=2, label="Savitzky-Golay", color='blue')
ax.axvline(x=w_critico, color='r', linestyle='--', alpha=0.7, label=f'w_critico ({w_critico:.6f})')
best_idx_sg = np.argmax(smoothed_savgol)
ax.plot(weights[best_idx_sg], smoothed_savgol[best_idx_sg], 'r*', markersize=15)
ax.set_title(f"Savitzky-Golay Filter (window={window}, poly={poly_order})", fontsize=14, fontweight='bold')
ax.set_xlabel("Average Synaptic Weight")
ax.set_ylabel("Test Accuracy")
ax.legend()
ax.grid(True, alpha=0.3)

# 3. Spline Smoothing (flexible, controlled by smoothing parameter)
ax = axes[1, 0]
spline = UnivariateSpline(weights, accuracies, s=0.01)  # s controls smoothness
weights_smooth = np.linspace(weights.min(), weights.max(), 1000)
accuracies_smooth = spline(weights_smooth)
ax.plot(weights, accuracies, 'o', alpha=0.3, markersize=2, label="Original", color='lightgreen')
ax.plot(weights_smooth, accuracies_smooth, '-', linewidth=2, label="Univariate Spline", color='green')
ax.axvline(x=w_critico, color='r', linestyle='--', alpha=0.7, label=f'w_critico ({w_critico:.6f})')
best_idx_sp = np.argmax(accuracies_smooth)
ax.plot(weights_smooth[best_idx_sp], accuracies_smooth[best_idx_sp], 'r*', markersize=15)
ax.set_title("Univariate Spline Smoothing (s=0.01)", fontsize=14, fontweight='bold')
ax.set_xlabel("Average Synaptic Weight")
ax.set_ylabel("Test Accuracy")
ax.legend()
ax.grid(True, alpha=0.3)

# 4. Moving Average (simple but effective)
ax = axes[1, 1]
window_ma = 25
smoothed_ma = np.convolve(accuracies, np.ones(window_ma)/window_ma, mode='valid')
weights_ma = weights[window_ma-1:]
ax.plot(weights, accuracies, 'o', alpha=0.3, markersize=2, label="Original", color='lightyellow')
ax.plot(weights_ma, smoothed_ma, '-', linewidth=2, label="Moving Average", color='orange')
ax.axvline(x=w_critico, color='r', linestyle='--', alpha=0.7, label=f'w_critico ({w_critico:.6f})')
best_idx_ma = np.argmax(smoothed_ma)
ax.plot(weights_ma[best_idx_ma], smoothed_ma[best_idx_ma], 'r*', markersize=15)
ax.set_title(f"Moving Average (window={window_ma})", fontsize=14, fontweight='bold')
ax.set_xlabel("Average Synaptic Weight")
ax.set_ylabel("Test Accuracy")
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig("lsm_smoothing_comparison.png", dpi=300, bbox_inches='tight')
print("Saved comparison plot as 'lsm_smoothing_comparison.png'")

# Create a final publication-ready plot with recommended smoothing
fig, ax = plt.subplots(1, 1, figsize=(12, 7))

# Use Savitzky-Golay as it preserves features well
smoothed = savgol_filter(accuracies, 51, 3)

ax.plot(weights, accuracies, 'o', alpha=0.2, markersize=3, color='steelblue', label="Raw Data")
ax.plot(weights, smoothed, '-', linewidth=2.5, color='darkblue', label="Smoothed (Savitzky-Golay)")
ax.axvline(x=w_critico, color='red', linestyle='--', linewidth=2, alpha=0.7, 
           label=f'Reference w_critico ({w_critico:.6f})')

best_idx_final = np.argmax(smoothed)
ax.plot(weights[best_idx_final], smoothed[best_idx_final], 'r*', markersize=20, 
        label=f'Peak Accuracy: {smoothed[best_idx_final]*100:.2f}% at w={weights[best_idx_final]:.6f}', 
        zorder=5)

ax.set_title("LSM Classifier Accuracy vs. Average Synaptic Weight", fontsize=16, fontweight='bold')
ax.set_xlabel("Average Synaptic Weight (mean_weight)", fontsize=13)
ax.set_ylabel("Test Accuracy", fontsize=13)
ax.legend(fontsize=11, loc='best')
ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.5)
ax.set_ylim([0.25, 0.80])

plt.tight_layout()
plt.savefig("lsm_smoothed_final.png", dpi=300, bbox_inches='tight')
print("Saved final smoothed plot as 'lsm_smoothed_final.png'")

plt.show()

print(f"\nâœ… Smoothing complete!")
print(f"Original best: {best_acc*100:.2f}% at w={best_w:.6f}")
print(f"Smoothed best: {smoothed[best_idx_final]*100:.2f}% at w={weights[best_idx_final]:.6f}")